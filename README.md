# Food Network

## Objective

I've always found it quite curious how remarkably similar foods from around the world are to each other -- consider samosas, patties, turnovers, empanadas, and fried dumplings, or the ubiquity of the onion. This study looks to 1) mathematically capture which ingredients are consistently used as building blocks to create dishes and 2) develop some measure for the strength of the bond between ingredients.

## Analysis

Consider a good sample (no pun intended) of recipes. It seems that if one were to treat each ingredient in the set of recipes as a vertex and each joint appearance in a recipe as an edge between two ingredients, a chaotic web replete with insight would emerge -- a food network. Dr. Mark Newman has developed a useful [algorithm](http://www.pnas.org/content/103/23/8577.full.pdf) that will serve as the main engine for analyzing such a network, yielding patterns in how certain foods are used together, and perhaps revealing foods that are consistently paired.

Upon building the network, one might be tempted to consider the element with the greatest value and label the corresponding pair as having the strongest bond. Such an approach will be highly biased towards ingredients that are frequently used and those that are used in recipes with many ingredients. Ultimately, the approach would tell us that salt and pepper go well together, but surely it is more likely the case that salt adds flavor to everything else *and* pepper adds flavor to everything else in a way that these two frequently appear together. Don't believe me? Try eating salt alone, then think about how much pepper improves the flavor. In more technical terms, the versatility of salt and pepper to independently flavor such a wide variety of foods creates something of a spurious correlation between the two. A more refined approach is informed by Newman's work.

Newman advocates comparing the actual linkages between nodes in a network against some baseline expectation. The matrix of joint appearances (the so-called adjacency matrix) serves as the "actual" and the "baseline expectation" is the random expectation given a sample space of network configurations with the same construction of degrees. The difference between the two yields a matrix, B, where each element answers the question "how well connected is this pair given the random expectation for this network", thus correcting the two aforementioned biases. This B matrix can be reframed as the linear combination of the normalized eigenvectors, and the network can be split into two based on the signs of the dominant eigenvector; those indices with positive values in the dominant eigenvector are in one group and those with negative values are in the other. This bifurcation reveals the natural division into the least overlapping groups.

Using a dataset of over 40,000 recipes from around the world made available by Yummly and Kaggle, I was able to observe over 6,700 ingredients used in a variety of combinations. With so many ingredients, even limiting the scope to groups of two, there are well over 22 million potential combinations to consider. As a result, tracking how all of the combinations (of any size) relate to each other was initially intractable, but Newman's work was a saving grace.

NB - Once I began to work with the data, it became clear very quickly that directly calculating the eigenvectors of the ~6,500x6,500 matrix was impractical. Instead, I've used power iteration to approximate the dominant eigenvector, which is sufficient for the algorithm. It is also important to note that there is a high variance in the number of ingredients. I discovered this importance by realizing that treating all linkages equally effectively biases the network to give the recipes with more ingredients more weight. In order to enforce that each recipe contributes equal weight to the model, I normalize the weights of the links such that the weight of each link within the recipe is equal and the sum of the weights for each recipe is one.

## Follow on work

A large part of moving this analysis forward will be resolving the following set of outstanding vulnerabilities:
* standardization of ingredients - there is much to be desired in terms of developing canonical names for ingredients. There are obvious cases to resolve such as "pepper" and "black pepper". There are also less obvious cases to consider such as "2% milk" and "whole milk"
* incorporate measurements - there is currently no attention paid to *how much* of an ingredient is used in a recipe
* slow convergence - ideally the bifurcation would be repeated until all atomic subgroups are found such that no further splits in the groups can be made. If the number of subgroups is large, a large number of iterations are required to reveal all the subgroups. At the extreme, each atomic subgroup could consist of two ingredients, yielding several thousand groups. The current implementation cannot support so many groups and iterations
